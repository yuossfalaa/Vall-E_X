Extra info for the current checkpoint :
NUM_LAYERS = 12
NUM_HEAD = 16
N_DIM = 1024
PREFIX_MODE = 1
NUM_QUANTIZERS = 8
SAMPLE_RATE = 24000


#To Download MGB2 First
cd DataSet

git lfs install

git clone https://huggingface.co/datasets/arbml/mgb2_speech

Token & Password = .....

cd ..


#download All needed requirement

$ pip install -r requirements.txt

#Download All needed Models

python3 utils/download.py



# Training starts here
exp_dir=exp/valle



#full training will be best at the start

#For Downloaded checkpoint run this :
cp /checkpoints/vallex-checkpoint.pt ${exp_dir}/epoch-1.pt  # --start-epoch should equal 2 as 2=1+1
Then Set start Epoch for Stage 0 or 1 To -> 2

#Train Stage 0

python3 bin/trainer.py --max-duration 40 --filter-min-duration 0.5 --filter-max-duration 14 --train-stage 0 --num-buckets 6 --save-every-n 10000 --valid-interval 20000 --model-name valle --share-embedding true --norm-first true --add-prenet false --decoder-dim 1024 --nhead 16 --num-decoder-layers 12 --prefix-mode 1 --base-lr 0.05 --warmup-steps 200 --average-period 0 --num-epochs 20 --start-epoch 2 --start-batch 0 --accumulate-grad-steps 4 --exp-dir ${exp_dir}

#More fine-tuning to AR and NAR Could be Done

#Train Stage 1

#if There's a Checkpoint
cp ${exp_dir}/best-valid-loss.pt ${exp_dir}/epoch-1.pt  # --start-epoch should equal 2 as 2=1+1

python3 bin/trainer.py --max-duration 40 --filter-min-duration 0.5 --filter-max-duration 14 --train-stage 1 --num-buckets 6 --save-every-n 10000 --valid-interval 20000 --model-name valle --share-embedding true --norm-first true --add-prenet false --decoder-dim 1024 --nhead 16 --num-decoder-layers 12 --prefix-mode 1 --base-lr 0.05 --warmup-steps 200 --average-period 0 --num-epochs 20 --start-epoch 2 --start-batch 0 --accumulate-grad-steps 4 --exp-dir ${exp_dir}


#Train Stage 2

cp ${exp_dir}/best-valid-loss.pt ${exp_dir}/epoch-2.pt  # --start-epoch 3=2+1

python3 bin/trainer.py --max-duration 40 --filter-min-duration 0.5 --filter-max-duration 14 --train-stage 2 --num-buckets 6 --save-every-n 10000 --valid-interval 20000 --model-name valle --share-embedding true --norm-first true --add-prenet false --decoder-dim 1024 --nhead 16 --num-decoder-layers 12 --prefix-mode 1 --base-lr 0.05 --warmup-steps 200 --average-period 0 --num-epochs 40 --start-epoch 3 --start-batch 0 --accumulate-grad-steps 4 --exp-dir ${exp_dir}





python3 -m bin.trainer --max-duration 80 --filter-min-duration 0.5 --filter-max-duration 14 --train-stage 1 --num-buckets 20 --dtype "bfloat16" --save-every-n 3000 --valid-interval 10000 --model-name valle --decoder-dim 1024 --nhead 16 --num-decoder-layers 12 --prefix-mode 1 --base-lr 0.05 --warmup-steps 200 --average-period 0 --num-epochs 20 --start-epoch 1 --start-batch 0 --accumulate-grad-steps 4 --exp-dir ${exp_dir} --keep-last-k 3

python3 -m bin.trainer --max-duration 40 --filter-min-duration 0.5 --filter-max-duration 14 --train-stage 2 --num-buckets 20 --dtype "float32"  --save-every-n 3000 --valid-interval 10000 --model-name valle --decoder-dim 1024 --nhead 16 --num-decoder-layers 12 --prefix-mode 1 --base-lr 0.05 --warmup-steps 200 --average-period 0 --num-epochs 40 --start-epoch 3 --start-batch 0 --accumulate-grad-steps 4 --exp-dir ${exp_dir} --keep-last-k 3





exp_dir=exp/valle

#full training will be best at the start

#Train Stage 0

python3 bin/trainer.py --max-duration 80 --filter-min-duration 0.5 --filter-max-duration 14 --train-stage 0 --num-buckets 6 --save-every-n 10000 --valid-interval 20000 --model-name valle --share-embedding true --norm-first true --add-prenet false --decoder-dim 1024 --nhead 16 --num-decoder-layers 12 --prefix-mode 1 --base-lr 0.05 --warmup-steps 200 --average-period 0 --num-epochs 20 --start-epoch 1 --start-batch 0 --accumulate-grad-steps 4 --exp-dir ${exp_dir}

#More fine-tuning to AR and NAR Could be Done

#Train Stage 1

#if There's a Checkpoint
cp ${exp_dir}/best-valid-loss.pt ${exp_dir}/epoch-1.pt  # --start-epoch should equal 2 as 2=1+1

python3 bin/trainer.py --max-duration 80 --filter-min-duration 0.5 --filter-max-duration 14 --train-stage 1 --num-buckets 6 --save-every-n 10000 --valid-interval 20000 --model-name valle --share-embedding true --norm-first true --add-prenet false --decoder-dim 1024 --nhead 16 --num-decoder-layers 12 --prefix-mode 1 --base-lr 0.05 --warmup-steps 200 --average-period 0 --num-epochs 20 --start-epoch 1 --start-batch 0 --accumulate-grad-steps 4 --exp-dir ${exp_dir}


#Train Stage 2

cp ${exp_dir}/best-valid-loss.pt ${exp_dir}/epoch-2.pt  # --start-epoch 3=2+1

python3 bin/trainer.py --max-duration 40 --filter-min-duration 0.5 --filter-max-duration 14 --train-stage 2 --num-buckets 6 --save-every-n 10000 --valid-interval 20000 --model-name valle --share-embedding true --norm-first true --add-prenet false --decoder-dim 1024 --nhead 16 --num-decoder-layers 12 --prefix-mode 1 --base-lr 0.05 --warmup-steps 200 --average-period 0 --num-epochs 40 --start-epoch 3 --start-batch 0 --accumulate-grad-steps 4 --exp-dir ${exp_dir}




